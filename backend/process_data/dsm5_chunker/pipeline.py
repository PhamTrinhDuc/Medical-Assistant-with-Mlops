import os
import re
from typing import Any, Dict, List
import pdfplumber

# ============================================================
# CONFIGURATION
# ============================================================
MIN_CHUNK_SIZE = 200  # Chunk nhá» hÆ¡n sáº½ Ä‘Æ°á»£c merge
MAX_CHUNK_SIZE = 1500  # Chunk lá»›n hÆ¡n sáº½ Ä‘Æ°á»£c split
TARGET_CHUNK_SIZE = 800  # KÃ­ch thÆ°á»›c má»¥c tiÃªu

# ============================================================
# PATTERNS
# ============================================================
# Pattern cho section header: "1.2.3 TÃªn section"
SECTION_PATTERN = re.compile(r"^(\d+(?:\.\d+)*)\s+([A-ZÃ€-á»¸a-zÃ -á»¹].*)")

# Pattern cho tiÃªu chÃ­ cháº©n Ä‘oÃ¡n: "A.", "B.", "C."
CRITERIA_PATTERN = re.compile(r"^([A-Z])\.\s+(.+)", re.DOTALL)

# Pattern cho má»¥c con: "1.", "2.", "3." hoáº·c "a.", "b.", "c."
SUB_CRITERIA_PATTERN = re.compile(r"^(\d+|[a-z])\.\s+(.+)", re.DOTALL)

# Pattern cho pháº§n "Cháº©n Ä‘oÃ¡n phÃ¢n biá»‡t"
DIFF_DIAG_PATTERN = re.compile(
    r"(Cháº©n Ä‘oÃ¡n phÃ¢n biá»‡t|cháº©n Ä‘oÃ¡n phÃ¢n biá»‡t)[:\s]*", re.IGNORECASE
)

# Pattern cho sá»‘ trang footer (vÃ­ dá»¥: "12 Chá»‰ sá»­ dá»¥ng tÃ i liá»‡u...")
PAGE_FOOTER_PATTERN = re.compile(r"^\d+\s+[Cc]há»‰ sá»­ dá»¥ng tÃ i liá»‡u.*$")


def smart_join_lines(lines: list[str]) -> str:
    """
    GhÃ©p cÃ¡c dÃ²ng PDF thÃ nh cÃ¡c Ä‘oáº¡n logic:
    - KhÃ´ng ghÃ©p náº¿u dÃ²ng trÆ°á»›c káº¿t thÃºc báº±ng dáº¥u cÃ¢u (. ! ? : ) â€¦
    - KhÃ´ng ghÃ©p náº¿u dÃ²ng sau báº¯t Ä‘áº§u báº±ng tiÃªu chÃ­ (A., 1., v.v.)
    - Bá» qua footer trang
    """
    if not lines:
        return ""

    # Lá»c bá» footer trang
    filtered_lines = [l for l in lines if not PAGE_FOOTER_PATTERN.match(l.strip())]
    if not filtered_lines:
        return ""

    paragraphs = []
    current_para = filtered_lines[0]

    for i in range(1, len(filtered_lines)):
        prev_line = current_para.strip()
        curr_line = filtered_lines[i].strip()

        # Náº¿u dÃ²ng trÆ°á»›c káº¿t thÃºc báº±ng dáº¥u káº¿t thÃºc => ngáº¯t Ä‘oáº¡n
        if re.search(r"[.!?â€¦:)]$", prev_line):
            paragraphs.append(current_para)
            current_para = curr_line

        # Náº¿u dÃ²ng sau báº¯t Ä‘áº§u báº±ng tiÃªu chÃ­ => ngáº¯t (A., B., 1., 2., v.v.)
        elif re.match(r"^[A-Z]\.\s", curr_line) or re.match(r"^\d+\.\s", curr_line):
            paragraphs.append(current_para)
            current_para = curr_line

        else:
            current_para += " " + curr_line

    paragraphs.append(current_para)
    return "\n".join(paragraphs)


def add_parent_title(chunks: list[dict]) -> list[dict]:
    """
    ThÃªm parent_title cho má»—i chunk dá»±a trÃªn parent_id.

    FIX: Sá»­ dá»¥ng unique_id thay vÃ¬ section_id vÃ¬ PDF cÃ³ thá»ƒ cÃ³
    nhiá»u section cÃ¹ng ID á»Ÿ cÃ¡c pháº§n khÃ¡c nhau.
    """
    # Build map tá»« unique_id -> chunk
    # unique_id Ä‘Æ°á»£c gÃ¡n trong extract function
    chunk_map = {}
    for c in chunks:
        uid = c.get("unique_id")
        if uid:
            chunk_map[uid] = c

    for chunk in chunks:
        parent_uid = chunk.get("parent_unique_id")
        if parent_uid and parent_uid in chunk_map:
            parent_chunk = chunk_map[parent_uid]
            chunk["parent_title"] = parent_chunk.get("title", "")
        else:
            chunk["parent_title"] = None

    return chunks


def clean_text(text: str) -> str:
    """
    Clean and normalize text.
    """
    if not text or not isinstance(text, str):
        return ""

    # Remove extra whitespace
    text = re.sub(r"\s+", " ", text)
    # Remove special characters but keep Vietnamese
    text = re.sub(r"[^\w\s\u00C0-\u1EF9.,;:!?()/-]", "", text)
    return text.strip()


def build_context_header(chunk: dict, chunk_map: dict) -> str:
    """
    XÃ¢y dá»±ng context header cho chunk.
    VÃ­ dá»¥: "[1 Rá»‘i loáº¡n phÃ¡t triá»ƒn tháº§n kinh > 1.3 Rá»‘i loáº¡n phá»• tá»± ká»·]"

    FIX: Sá»­ dá»¥ng unique_id thay vÃ¬ section_id
    """
    path_parts = []
    current = chunk

    # Traverse up the hierarchy
    visited = set()
    while current:
        unique_id = current.get("unique_id")
        if unique_id in visited:
            break
        visited.add(unique_id)

        title = current.get("title", "")
        if title:
            # Láº¥y pháº§n tiÃªu Ä‘á» ngáº¯n gá»n (bá» pháº§n chi tiáº¿t sau dáº¥u :)
            short_title = title.split(":")[0].strip()[:60]
            path_parts.insert(0, short_title)

        parent_uid = current.get("parent_unique_id")
        if parent_uid and parent_uid in chunk_map:
            current = chunk_map[parent_uid]
        else:
            break

    if path_parts:
        return "[" + " > ".join(path_parts) + "]"
    return ""


def split_long_content(text: str) -> List[Dict[str, Any]]:
    """
    Split ná»™i dung dÃ i thÃ nh cÃ¡c chunks nhá» hÆ¡n.

    Chiáº¿n lÆ°á»£c:
    1. Æ¯u tiÃªn split theo tiÃªu chÃ­ cháº©n Ä‘oÃ¡n (A., B., C., ...)
    2. Náº¿u váº«n cÃ²n dÃ i, split theo má»¥c con (1., 2., 3., ...)
    3. Cuá»‘i cÃ¹ng, split theo cÃ¢u
    """
    sub_chunks = []

    # Thá»­ split theo tiÃªu chÃ­ chÃ­nh (A., B., C., ...)
    criteria_parts = re.split(r"(?=\n[A-Z]\.\s)", text)
    if len(criteria_parts) > 1:
        # CÃ³ nhiá»u tiÃªu chÃ­, split theo tá»«ng tiÃªu chÃ­
        for i, part in enumerate(criteria_parts):
            part = part.strip()
            if not part:
                continue

            # TÃ¬m label cá»§a tiÃªu chÃ­
            match = re.match(r"^([A-Z])\.\s", part)
            if match:
                criteria_label = match.group(1)
                sub_id = f"criteria_{criteria_label}"
                sub_title = f"TiÃªu chÃ­ {criteria_label}"
            else:
                sub_id = f"intro"
                sub_title = "Giá»›i thiá»‡u"

            # Náº¿u pháº§n nÃ y váº«n cÃ²n quÃ¡ dÃ i, split tiáº¿p theo cÃ¢u
            if len(part) > MAX_CHUNK_SIZE:
                sentence_chunks = split_by_sentences(part, MAX_CHUNK_SIZE)
                for j, sent_chunk in enumerate(sentence_chunks):
                    sub_chunks.append(
                        {
                            "content": sent_chunk,
                            "sub_id": f"{sub_id}_p{j+1}",
                            "sub_title": f"{sub_title} (pháº§n {j+1})",
                        }
                    )
            else:
                sub_chunks.append(
                    {"content": part, "sub_id": sub_id, "sub_title": sub_title}
                )
    else:
        # KhÃ´ng cÃ³ tiÃªu chÃ­ A/B/C, thá»­ split theo má»¥c con (1., 2., ...)
        sub_parts = re.split(r"(?=\n\d+\.\s)", text)

        if len(sub_parts) > 1 and all(len(p) < MAX_CHUNK_SIZE for p in sub_parts):
            for i, part in enumerate(sub_parts):
                part = part.strip()
                if not part:
                    continue

                match = re.match(r"^(\d+)\.\s", part)
                if match:
                    item_num = match.group(1)
                    sub_id = f"item_{item_num}"
                    sub_title = f"Má»¥c {item_num}"
                else:
                    sub_id = f"intro"
                    sub_title = "Giá»›i thiá»‡u"

                sub_chunks.append(
                    {"content": part, "sub_id": sub_id, "sub_title": sub_title}
                )
        else:
            # Fallback: split theo cÃ¢u
            sentence_chunks = split_by_sentences(text, MAX_CHUNK_SIZE)
            for j, sent_chunk in enumerate(sentence_chunks):
                sub_chunks.append(
                    {
                        "content": sent_chunk,
                        "sub_id": f"part_{j+1}",
                        "sub_title": f"Pháº§n {j+1}",
                    }
                )

    return (
        sub_chunks
        if sub_chunks
        else [{"content": text, "sub_id": None, "sub_title": None}]
    )


def split_by_sentences(text: str, max_size: int) -> List[str]:
    """
    Split text theo cÃ¢u, Ä‘áº£m báº£o má»—i chunk khÃ´ng vÆ°á»£t quÃ¡ max_size.
    """
    # Split theo dáº¥u cháº¥m, nhÆ°ng giá»¯ nguyÃªn cÃ¢u
    sentences = re.split(r"(?<=[.!?])\s+", text)

    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if not sentence.strip():
            continue

        potential = current_chunk + " " + sentence if current_chunk else sentence

        if len(potential) <= max_size:
            current_chunk = potential
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence

    if current_chunk.strip():
        chunks.append(current_chunk.strip())

    return chunks if chunks else [text]


def convert_to_documents(
    chunks: list[dict], apply_split: bool = True
) -> List[Dict[str, Any]]:
    """
    Chuyá»ƒn Ä‘á»•i chunks thÃ nh documents vá»›i:
    1. Context header
    2. Split chunks dÃ i
    3. Merge chunks ngáº¯n

    FIX: Sá»­ dá»¥ng unique_id thay vÃ¬ section_id cho chunk_map
    """
    # Build chunk_map Ä‘á»ƒ lookup parent (dÃ¹ng unique_id)
    chunk_map = {}
    for c in chunks:
        uid = c.get("unique_id")
        if uid:
            chunk_map[uid] = c

    documents = []
    doc_index = 1

    for chunk in chunks:
        title = chunk.get("title", "")
        text = chunk.get("text", "")
        full_content = f"{title}\n{text}".strip()

        # Build context header
        context_header = build_context_header(chunk, chunk_map)

        if apply_split and len(full_content) > MAX_CHUNK_SIZE:
            # Split chunk dÃ i
            sub_chunks = split_long_content(text, title)

            for sub in sub_chunks:
                sub_content = sub["content"]
                sub_title = sub.get("sub_title", "")
                sub_id = sub.get("sub_id", "")

                # ThÃªm context header
                final_content = (
                    f"{context_header}\n{sub_content}"
                    if context_header
                    else sub_content
                )
                cleaned_content = clean_text(final_content)

                if len(cleaned_content) < MIN_CHUNK_SIZE:
                    continue  # Bá» qua chunk quÃ¡ ngáº¯n

                doc = {
                    "index": doc_index,
                    "section_id": chunk["section_id"],
                    "sub_id": sub_id,
                    "level": chunk["level"],
                    "parent_id": chunk.get("parent_id"),
                    "parent_title": chunk.get("parent_title"),
                    "title": title,
                    "sub_title": sub_title,
                    "context_header": context_header,
                    "content": cleaned_content,
                    "content_raw": sub_content,
                    "metadata": {
                        "page_start": chunk["page_start"],
                        "source": chunk["source"],
                        "char_count": len(cleaned_content),
                        "is_split": True,
                    },
                }
                documents.append(doc)
                doc_index += 1
        else:
            # Chunk Ä‘á»§ ngáº¯n hoáº·c khÃ´ng cáº§n split
            final_content = (
                f"{context_header}\n{full_content}" if context_header else full_content
            )
            cleaned_content = clean_text(final_content)

            # ÄÃ¡nh dáº¥u chunk ngáº¯n
            is_short = len(cleaned_content) < MIN_CHUNK_SIZE

            doc = {
                "index": doc_index,
                "section_id": chunk["section_id"],
                "sub_id": None,
                "level": chunk["level"],
                "parent_id": chunk.get("parent_id"),
                "parent_title": chunk.get("parent_title"),
                "title": title,
                "sub_title": None,
                "context_header": context_header,
                "content": cleaned_content,
                "content_raw": text,
                "metadata": {
                    "page_start": chunk["page_start"],
                    "source": chunk["source"],
                    "char_count": len(cleaned_content),
                    "is_split": False,
                    "is_short": is_short,
                },
            }
            documents.append(doc)
            doc_index += 1

    # Merge short chunks with next sibling
    documents = merge_short_chunks(documents)

    # Re-index
    for i, doc in enumerate(documents, 1):
        doc["index"] = i

    return documents


def merge_short_chunks(documents: List[Dict]) -> List[Dict]:
    """
    Merge cÃ¡c chunk ngáº¯n vá»›i chunk tiáº¿p theo cÃ¹ng parent.
    """
    if not documents:
        return documents

    merged = []
    i = 0

    while i < len(documents):
        current = documents[i]

        # Kiá»ƒm tra náº¿u chunk ngáº¯n vÃ  cÃ³ thá»ƒ merge
        if current["metadata"].get("is_short", False) and i + 1 < len(documents):

            next_doc = documents[i + 1]

            # Chá»‰ merge náº¿u cÃ¹ng parent
            if current.get("parent_id") == next_doc.get("parent_id"):
                combined_content = current["content"] + "\n\n" + next_doc["content"]

                if len(combined_content) <= MAX_CHUNK_SIZE:
                    # Merge
                    merged_doc = {
                        **next_doc,
                        "content": combined_content,
                        "title": current["title"] + " + " + next_doc["title"],
                        "metadata": {
                            **next_doc["metadata"],
                            "char_count": len(combined_content),
                            "merged_from": [
                                current["section_id"],
                                next_doc["section_id"],
                            ],
                        },
                    }
                    merged.append(merged_doc)
                    i += 2
                    continue

        merged.append(current)
        i += 1

    return merged


def extract_dsm_chunk_hierarchical(pdf_path: str) -> List[Dict[str, Any]]:
    """
    TrÃ­ch xuáº¥t vÃ  chunk tÃ i liá»‡u DSM-5 tiáº¿ng Viá»‡t theo cáº¥u trÃºc phÃ¢n cáº¥p.

    Cáº£i tiáº¿n:
    1. Fix parent_id tracking - xÃ³a level cao hÆ¡n khi gáº·p section má»›i
    2. Lá»c footer trang
    3. Sá»­ dá»¥ng unique_id Ä‘á»ƒ trÃ¡nh conflict khi cÃ³ duplicate section_id
    """
    chunks = []
    current_chunk = None
    buffer_lines = []

    # LÆ°u má»¥c gáº§n nháº¥t á»Ÿ má»—i cáº¥p Ä‘á»™: {level: unique_id}
    last_section_at_level: Dict[int, str] = {}

    # Counter Ä‘á»ƒ táº¡o unique_id
    chunk_counter = 0

    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages, 1):
            text = page.extract_text(x_tolerance=1, y_tolerance=1)
            if not text or not text.strip():
                continue

            lines = text.split("\n")

            for line in lines:
                line = line.strip()
                if not line:
                    continue

                # Bá» qua footer trang
                if PAGE_FOOTER_PATTERN.match(line):
                    continue

                matched = SECTION_PATTERN.match(line)

                if matched:
                    # LÆ°u chunk hiá»‡n táº¡i (náº¿u cÃ³)
                    if current_chunk is not None:
                        if buffer_lines:
                            current_chunk["text"] = smart_join_lines(buffer_lines)
                        chunks.append(current_chunk)
                        buffer_lines = []

                    section_id = matched.group(1)
                    level = len(section_id.split("."))

                    # Táº¡o unique_id
                    chunk_counter += 1
                    unique_id = f"chunk_{chunk_counter}"

                    # âœ… FIX: XÃ³a cÃ¡c level >= level hiá»‡n táº¡i
                    # Äiá»u nÃ y Ä‘áº£m báº£o parent_id luÃ´n chÃ­nh xÃ¡c
                    keys_to_remove = [k for k in last_section_at_level if k >= level]
                    for k in keys_to_remove:
                        del last_section_at_level[k]

                    # LÆ°u unique_id cá»§a level hiá»‡n táº¡i
                    last_section_at_level[level] = unique_id

                    # TÃ¬m parent_unique_id (level gáº§n nháº¥t tháº¥p hÆ¡n)
                    parent_unique_id = None
                    for l in range(level - 1, 0, -1):
                        if l in last_section_at_level:
                            parent_unique_id = last_section_at_level[l]
                            break

                    current_chunk = {
                        "unique_id": unique_id,
                        "section_id": section_id,
                        "level": level,
                        "parent_id": (
                            section_id.rsplit(".", 1)[0] if "." in section_id else None
                        ),  # Váº«n giá»¯ cho reference
                        "parent_unique_id": parent_unique_id,
                        "title": line,
                        "text": "",
                        "page_start": page_num,
                        "source": os.path.abspath(pdf_path),
                    }

                else:
                    # KhÃ´ng pháº£i title â†’ thÃªm vÃ o buffer
                    if current_chunk is not None:
                        buffer_lines.append(line)

        # Xá»­ lÃ½ chunk cuá»‘i cÃ¹ng
        if current_chunk is not None:
            if buffer_lines:
                current_chunk["text"] = smart_join_lines(buffer_lines)
            chunks.append(current_chunk)

    # ThÃªm parent_title
    chunks = add_parent_title(chunks)

    # Convert sang documents vá»›i split/merge
    documents = convert_to_documents(chunks, apply_split=True)

    return documents


def print_statistics(documents: List[Dict]) -> None:
    """In thá»‘ng kÃª vá» chunks."""
    if not documents:
        print("KhÃ´ng cÃ³ chunks!")
        return

    sizes = [doc["metadata"]["char_count"] for doc in documents]

    print("\n" + "=" * 60)
    print("ğŸ“Š THá»NG KÃŠ CHUNKS")
    print("=" * 60)
    print(f"Tá»•ng sá»‘ chunks: {len(documents)}")
    print(f"KÃ­ch thÆ°á»›c trung bÃ¬nh: {sum(sizes)/len(sizes):.0f} kÃ½ tá»±")
    print(f"KÃ­ch thÆ°á»›c nhá» nháº¥t: {min(sizes)} kÃ½ tá»±")
    print(f"KÃ­ch thÆ°á»›c lá»›n nháº¥t: {max(sizes)} kÃ½ tá»±")

    # PhÃ¢n bá»‘ theo size
    short = sum(1 for s in sizes if s < MIN_CHUNK_SIZE)
    medium = sum(1 for s in sizes if MIN_CHUNK_SIZE <= s <= MAX_CHUNK_SIZE)
    long = sum(1 for s in sizes if s > MAX_CHUNK_SIZE)

    print(f"\nPhÃ¢n bá»‘ kÃ­ch thÆ°á»›c:")
    print(f"  - Ngáº¯n (<{MIN_CHUNK_SIZE}): {short}")
    print(f"  - Vá»«a ({MIN_CHUNK_SIZE}-{MAX_CHUNK_SIZE}): {medium}")
    print(f"  - DÃ i (>{MAX_CHUNK_SIZE}): {long}")

    # Theo level
    level_counts: Dict[int, int] = {}
    for doc in documents:
        lvl = doc["level"]
        level_counts[lvl] = level_counts.get(lvl, 0) + 1

    print(f"\nTheo cáº¥p Ä‘á»™:")
    for lvl in sorted(level_counts.keys()):
        print(f"  - Level {lvl}: {level_counts[lvl]}")


if __name__ == "__main__":
    PDF_PATH = "/home/ducpham/workspace/LLM-Chatbot-with-LangChain-and-Neo4j/data/dsm-5-cac-tieu-chuan-chan-doan.pdf"

    print("ğŸ”„ Äang xá»­ lÃ½ PDF...")
    chunks = extract_dsm_chunk_hierarchical(PDF_PATH)

    # In thá»‘ng kÃª
    print_statistics(chunks)

    # In 5 chunks Ä‘áº§u tiÃªn Ä‘á»ƒ kiá»ƒm tra
    print("\n" + "=" * 60)
    print("ğŸ“ MáºªU 5 CHUNKS Äáº¦U TIÃŠN")
    print("=" * 60)
    for chunk in chunks[:5]:
        print(f"\n--- Chunk {chunk['index']} ---")
        print(f"Section ID: {chunk['section_id']}")
        print(f"Level: {chunk['level']}")
        print(f"Parent: {chunk.get('parent_title', 'None')}")
        print(f"Title: {chunk['title'][:50]}...")
        print(
            f"Context: {chunk['context_header'][:80]}..."
            if chunk.get("context_header")
            else "Context: None"
        )
        print(f"Size: {chunk['metadata']['char_count']} chars")
        print(f"Content preview: {chunk['content'][:150]}...")

    # LÆ°u JSON
    import json

    with open("dsm5_chunks.json", "w", encoding="utf-8") as f:
        json.dump(chunks, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… ÄÃ£ lÆ°u {len(chunks)} chunks vÃ o dsm5_chunks.json")
